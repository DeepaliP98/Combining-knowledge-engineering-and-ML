{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense,GRU,Input\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import keras_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_tweets.json\",'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 30\n",
    "EMBEDDING_SIZE = 300  \n",
    "TEST_SIZE = 0.25 \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(text):\n",
    "\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    # convert text to lower-case\n",
    "    nopunc = nopunc.lower()\n",
    "    # remove URLs\n",
    "    nopunc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', nopunc)\n",
    "    nopunc = re.sub(r'http\\S+', '', nopunc)\n",
    "    # remove usernames\n",
    "    nopunc = re.sub('@[^\\s]+', '', nopunc)\n",
    "    # remove the # in #hashtag\n",
    "    nopunc = re.sub(r'#([^\\s]+)', r'\\1', nopunc)\n",
    "    # remove repeated characters\n",
    "    nopunc = word_tokenize(nopunc)\n",
    "    # remove stopwords from final word list\n",
    "    return [word for word in nopunc if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet': 'No comparison #superbowl http://t.co/DV91J3zA', 'label': 0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed = []\n",
    "for i in range(0,len(data)):\n",
    "    pre_processed.append(preprocess_tweet(data[i]['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pre_processed)\n",
    "process = tokenizer.texts_to_sequences(pre_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for l in range(0,len(data)):\n",
    "    y.append(data[l]['label'])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 30\n",
    "# convert to numpy arrays\n",
    "process = np.array(process)\n",
    "y = np.array(y)\n",
    "process = pad_sequences(process, maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(process, y, test_size=TEST_SIZE, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(tokenizer, dim=300):\n",
    "    embedding_index = {}\n",
    "    with open(\"numberbatch-en.txt\", encoding='utf8') as f:\n",
    "        for line in tqdm.tqdm(f, \"Reading Numberbatch\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_index[word] = vectors\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found will be 0s\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(tokenizer, lstm_units):\n",
    "    \"\"\"\n",
    "    Constructs the model,\n",
    "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
    "    \"\"\"\n",
    "    # get the GloVe embedding vectors\n",
    "    embedding_matrix = get_embedding_vectors(tokenizer)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index)+1,\n",
    "              EMBEDDING_SIZE,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              input_length=SEQUENCE_LENGTH))\n",
    "\n",
    "    model.add(LSTM(lstm_units, recurrent_dropout=0.2))\n",
    "#     model.add(Dropout(0.3))\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "    # compile as rmsprop optimizer\n",
    "    # aswell as with recall metric\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Numberbatch: 516783it [00:43, 11962.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 30, 300)           5849100   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 6,069,135\n",
      "Trainable params: 220,035\n",
      "Non-trainable params: 5,849,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(tokenizer=tokenizer, lstm_units=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (9000, 30)\n",
      "X_test.shape: (3000, 30)\n",
      "y_train.shape: (9000, 3)\n",
      "y_test.shape: (3000, 3)\n",
      "Train on 9000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.3342 - acc: 0.8986 - precision: 0.9946 - recall: 0.8705 - val_loss: 0.1466 - val_acc: 0.9470 - val_precision: 0.9951 - val_recall: 0.9883\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.1173 - acc: 0.9573 - precision: 0.9980 - recall: 0.9882 - val_loss: 0.1106 - val_acc: 0.9710 - val_precision: 0.9980 - val_recall: 0.9903\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0937 - acc: 0.9688 - precision: 0.9986 - recall: 0.9916 - val_loss: 0.0986 - val_acc: 0.9697 - val_precision: 0.9980 - val_recall: 0.9912\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0803 - acc: 0.9720 - precision: 0.9986 - recall: 0.9929 - val_loss: 0.0876 - val_acc: 0.9700 - val_precision: 0.9990 - val_recall: 0.9893\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0742 - acc: 0.9744 - precision: 0.9990 - recall: 0.9939 - val_loss: 0.1034 - val_acc: 0.9643 - val_precision: 0.9951 - val_recall: 0.9912\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 9s 1ms/step - loss: 0.0666 - acc: 0.9781 - precision: 0.9990 - recall: 0.9929 - val_loss: 0.0820 - val_acc: 0.9757 - val_precision: 0.9990 - val_recall: 0.9932\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.0605 - acc: 0.9810 - precision: 0.9993 - recall: 0.9950 - val_loss: 0.0926 - val_acc: 0.9683 - val_precision: 0.9990 - val_recall: 0.9903\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.0569 - acc: 0.9808 - precision: 0.9990 - recall: 0.9953 - val_loss: 0.0808 - val_acc: 0.9730 - val_precision: 0.9961 - val_recall: 0.9942\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.0560 - acc: 0.9822 - precision: 0.9993 - recall: 0.9946 - val_loss: 0.1107 - val_acc: 0.9617 - val_precision: 0.9990 - val_recall: 0.9864\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 10s 1ms/step - loss: 0.0482 - acc: 0.9844 - precision: 0.9997 - recall: 0.9963 - val_loss: 0.0797 - val_acc: 0.9747 - val_precision: 0.9990 - val_recall: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5401f1a9b0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "# train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 1s 279us/step\n",
      "[+] Accuracy:  97.46666666666667\n",
      "[+] Precision:  99.90205679726719\n",
      "[+] Recall:  99.31840310620073\n"
     ]
    }
   ],
   "source": [
    "# get the loss and metrics\n",
    "result = model.evaluate(X_test, y_test)\n",
    "# extract those\n",
    "loss = result[0]\n",
    "accuracy = result[1]\n",
    "precision = result[2]\n",
    "recall = result[3]\n",
    "\n",
    "print(\"[+] Accuracy: \" , accuracy*100)\n",
    "print(\"[+] Precision: \" , precision*100)\n",
    "print(\"[+] Recall: \" , recall*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
